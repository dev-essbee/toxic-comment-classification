{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../models/1706293064.2710369'\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path}/params.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "max_length = params['max_length']\n",
    "padding_type = params['padding_type']\n",
    "vocab_size = params['vocab_size']\n",
    "embedding_dim = params['embedding_dim']\n",
    "trunc_type = params['trunc_type']\n",
    "oov_tok = params['oov_tok']\n",
    "model_accuracy=params['model_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=os.path.join(path,str(model_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using final model\n",
    "model = tf.keras.models.load_model(f'{path}.keras')\n",
    "history=json.load(open(f'{path}.json','r'))\n",
    "word_index=json.load(open(f'../processed/word_index.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = np.load('../processed/val_padded.npy')\n",
    "val_y = np.load('../processed/val_y.npy')\n",
    "complete_y=pd.read_csv('../kaggle_data/val_y.csv')\n",
    "complete_val_x=pd.read_csv('../kaggle_data/val_x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history[string])\n",
    "  plt.plot(history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get the weights of the embedding layer\n",
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Create a dictionary where the keys are the values from word_index and the values are the keys from word_index\n",
    "index_to_word = {value: key for key, value in word_index.items()}\n",
    "\n",
    "# Open two files for writing: one for the vectors and one for the metadata\n",
    "with io.open('./plot_embeddings/vectors.tsv', 'w', encoding='utf-8') as vectors_file, \\\n",
    "     io.open('./plot_embeddings/metadata.tsv', 'w', encoding='utf-8') as metadata_file:\n",
    "    # For each word in the vocabulary\n",
    "    for index in range(1, vocab_size):\n",
    "        # Get the word and its embedding\n",
    "        word = index_to_word[index]\n",
    "        embedding = weights[index]\n",
    "        # Write the word to the metadata file\n",
    "        metadata_file.write(word + \"\\n\")\n",
    "        # Write the embedding to the vectors file\n",
    "        vectors_file.write('\\t'.join([str(x) for x in embedding]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_worst_group_accuracy(predictions, labels):\n",
    "    # Add predictions to labels DataFrame\n",
    "    labels.loc[:, 'pred'] = predictions.pred\n",
    "\n",
    "    # Define the categories to consider\n",
    "    categories = ['male', 'female', 'LGBTQ', 'christian', 'muslim', 'other_religions', 'black', 'white']\n",
    "\n",
    "    # Initialize lists to store accuracies and group names\n",
    "    accuracies = []\n",
    "    group_names = []\n",
    "\n",
    "    # For each category\n",
    "    for category in categories:\n",
    "        # For each label in the category\n",
    "        for label in [0, 1]:\n",
    "            # Select the group with the current category and label\n",
    "            group = labels.loc[labels[category] == label]\n",
    "            # Calculate the accuracy of the predictions for this group\n",
    "            group_accuracy = (group['y'] == (group['pred'] > 0.5)).mean()\n",
    "            # Append the group name and accuracy to the respective lists\n",
    "            group_names.append(f'{category}_{label}')\n",
    "            accuracies.append(group_accuracy)\n",
    "\n",
    "    # Calculate the worst group accuracy\n",
    "    worst_group_accuracy = np.min(accuracies)\n",
    "\n",
    "    return worst_group_accuracy, group_names, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, loss_function):\n",
    "    # Initialize lists to store losses, predictions, and indices\n",
    "    batch_losses, all_predictions, batch_indices = [], [], []\n",
    "\n",
    "    # Iterate over batches in the data loader\n",
    "    for batch_idx, (inputs, targets) in tqdm(enumerate(data_loader), leave=False):\n",
    "        # Make predictions using the model\n",
    "        predictions = model(inputs, training=False)\n",
    "        # Calculate the loss\n",
    "        batch_loss = loss_function(targets, tf.squeeze(predictions))\n",
    "        # Extend the losses list with the current batch loss repeated for each target in the batch\n",
    "        batch_losses.extend([batch_loss.numpy()] * len(targets))\n",
    "        # Extend the predictions list with the current batch predictions\n",
    "        all_predictions.extend(tf.squeeze(predictions).numpy().tolist())\n",
    "        # Extend the indices list with the current batch index repeated for each target in the batch\n",
    "        batch_indices.extend([batch_idx] * len(targets))\n",
    "\n",
    "    # Create a DataFrame with the batch indices and predictions\n",
    "    predictions_df = pd.DataFrame({'index': batch_indices, 'pred': all_predictions})\n",
    "    # Calculate the mean loss over the entire dataset\n",
    "    mean_loss = np.mean(batch_losses)\n",
    "\n",
    "    return predictions_df, mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = tf.keras.losses.BinaryCrossentropy()\n",
    "val_x = val_x.reshape((val_x.shape[0], -1))\n",
    "data=tf.data.Dataset.from_tensor_slices((val_x,val_y))\n",
    "data=data.batch(batch_size)\n",
    "\n",
    "pred_df, loss = evaluate_model(model, data, criterion)\n",
    "wga, groups, accuracies = calculate_worst_group_accuracy(pred_df, complete_y)\n",
    "\n",
    "wga_dict=dict(zip(groups,accuracies))\n",
    "wga_dict['loss']=str(loss)\n",
    "wga_dict['wga']=str(wga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pred_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred['pred']=(pred['pred']>0.5).astype(int)\n",
    "pred.drop('index',axis=1,inplace=True)\n",
    "pred=pd.concat([pred,complete_y],axis=1)\n",
    "pred=pd.concat([pred,complete_val_x],axis=1).drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=path.split('/0')[0]\n",
    "pred.to_csv(f'{temp}/validation_pred.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path}_wga.json','w') as f:\n",
    "    json.dump(wga_dict,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
