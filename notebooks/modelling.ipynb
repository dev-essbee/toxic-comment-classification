{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../params.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "max_length = params['max_length']\n",
    "padding_type = params['padding_type']\n",
    "vocab_size = params['vocab_size']\n",
    "embedding_dim = params['embedding_dim']\n",
    "trunc_type = params['trunc_type']\n",
    "oov_tok = params['oov_tok']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../models/1706293064.2710369'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index=json.load(open(f'../processed/word_index.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=np.load('../processed/train_padded.npy')\n",
    "train_y=np.load('../processed/train_y.npy')\n",
    "val_x=np.load('../processed/val_padded.npy')\n",
    "val_y=np.load('../processed/val_y.npy')\n",
    "embeddings=np.load('../glove_embeddings_200d.npy')\n",
    "train_y_meta=pd.read_csv('../processed/train_meta.csv')\n",
    "val_y_meta=pd.read_csv('../processed/val_meta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=tf.keras.Sequential([ \n",
    "#     tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "#     tf.keras.layers.LSTM(100,return_sequences=True),\n",
    "#     tf.keras.layers.GlobalAveragePooling1D(),\n",
    "#     tf.keras.layers.Dropout(0.1),\n",
    "#     tf.keras.layers.Dense(50,activation='relu'),\n",
    "#     tf.keras.layers.Dropout(0.1),\n",
    "#     tf.keras.layers.Dense(50,activation='relu'),\n",
    "#     tf.keras.layers.Dropout(0.1),\n",
    "#     tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_worst_group_accuracy(labels, predictions):\n",
    "    # Add predictions to labels DataFrame\n",
    "    labels.loc[:, 'pred'] = predictions\n",
    "\n",
    "    # Define the categories to consider\n",
    "    categories = ['male', 'female', 'LGBTQ', 'christian', 'muslim', 'other_religions', 'black', 'white']\n",
    "\n",
    "    # Initialize lists to store accuracies and group names\n",
    "    group_accuracies = []\n",
    "    group_names = []\n",
    "\n",
    "    # For each category\n",
    "    for category in categories:\n",
    "        # For each label in the category\n",
    "        for label in [0, 1]:\n",
    "            # Select the group with the current category and label\n",
    "            group = labels.loc[labels[category] == label]\n",
    "            # Calculate the accuracy of the predictions for this group\n",
    "            group_accuracy = (group['y'] == (group['pred'] > 0.5)).mean()\n",
    "            # Append the group name and accuracy to the respective lists\n",
    "            group_names.append(f'{category}_{label}')\n",
    "            group_accuracies.append(group_accuracy)\n",
    "\n",
    "    # Calculate the worst group accuracy\n",
    "    worst_group_accuracy = np.min(group_accuracies)\n",
    "\n",
    "    # Return the worst group accuracy and a dictionary mapping group names to accuracies\n",
    "    return worst_group_accuracy, dict(zip(group_names, group_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_and_predict(input_data, meta_data, model):\n",
    "    # Reshape the input data to 2D\n",
    "    reshaped_input = input_data.reshape((input_data.shape[0], -1))\n",
    "\n",
    "    # Create a TensorFlow dataset from the reshaped input and meta data\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((reshaped_input, meta_data))\n",
    "\n",
    "    # Batch the dataset\n",
    "    batched_dataset = dataset.batch(32)\n",
    "\n",
    "    # Initialize a list to store predictions\n",
    "    predictions = []\n",
    "\n",
    "    # For each batch in the dataset\n",
    "    for batch_idx, (inputs, _) in tqdm(enumerate(batched_dataset), leave=False):\n",
    "        # Make predictions using the model\n",
    "        batch_predictions = model(inputs, training=False)\n",
    "        # Extend the predictions list with the current batch predictions\n",
    "        predictions.extend(tf.squeeze(batch_predictions).numpy().tolist())\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Keras Callback to calculate and print the worst group accuracy after each epoch\n",
    "class WorstGroupAccuracy(Callback):\n",
    "    # Initialize the callback with training and validation data\n",
    "    def __init__(self, training_data, validation_data):\n",
    "        super(WorstGroupAccuracy, self).__init__()\n",
    "        self.training_data = training_data\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    # This method is called at the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Unpack the validation data\n",
    "        validation_inputs, _, validation_meta_data = self.validation_data\n",
    "        \n",
    "        # Use the model to make predictions on the validation data\n",
    "        validation_predictions = batch_and_predict(validation_inputs, validation_meta_data, self.model)\n",
    "        \n",
    "        # Calculate the worst group accuracy and the metric for each group\n",
    "        worst_group_accuracy, group_metrics = calculate_worst_group_accuracy(validation_meta_data, validation_predictions)\n",
    "        \n",
    "        # Print the worst group accuracy and the metric for each group\n",
    "        print(f'{worst_group_accuracy}, Validation WGA: {group_metrics}')\n",
    "        \n",
    "# Instantiate the callback with training and validation data\n",
    "wga_callback = WorstGroupAccuracy((train_x, train_y, train_meta), (val_x, val_y, val_y_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates a custom loss function that takes into account demographic information\n",
    "def demographic_loss_wrapper(demography, loss_fn):\n",
    "    # The custom loss function\n",
    "    def demographic_loss(y_true, y_pred):\n",
    "        # Split the true labels and the demographic information\n",
    "        y_true, demography = tf.split(y_true, [1, 1], axis=-1)\n",
    "        \n",
    "        # Cast the demographic information to boolean\n",
    "        demography = tf.cast(demography, tf.bool)\n",
    "        \n",
    "        # Cast the true labels to float32\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        \n",
    "        # Create a tensor filled with 0.5 of the same shape as y_true\n",
    "        half_tensor = tf.fill(tf.shape(y_true), 0.5)\n",
    "        \n",
    "        # If the demographic information is True, set the true label to 0.5, otherwise keep the original value\n",
    "        y_true = tf.where(demography, half_tensor, y_pred)\n",
    "        \n",
    "        # Calculate the loss using the provided loss function\n",
    "        return loss_fn(y_true, y_pred)\n",
    "    \n",
    "    # Return the custom loss function\n",
    "    return demographic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sequential model\n",
    "model = tf.keras.Sequential([ \n",
    "    # Add an Embedding layer that turns positive integers (indexes) into dense vectors of fixed size\n",
    "    # The model will take as input an integer matrix of size (batch, input_length)\n",
    "    # The largest integer (i.e. word index) in the input should be no larger than vocab_size (vocabulary size)\n",
    "    # Now model.output_shape == (None, input_length, embedding_dim)\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embeddings], trainable=False),\n",
    "    \n",
    "    # Add a Bidirectional layer with a GRU layer that contains 50 units\n",
    "    # The output sequences from the GRU layer are returned as they are\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(50, return_sequences=True)),\n",
    "    \n",
    "    # Add a GlobalAveragePooling1D layer that averages over the sequence dimension (axis 1) to produce a fixed length output vector\n",
    "    # This allows the model to handle input of variable length in a simple and computationally efficient way\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    \n",
    "    # Add a Dropout layer that randomly sets 10% of input units to 0 at each update during training time\n",
    "    # This helps prevent overfitting\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    \n",
    "    # Add a Dense layer with 50 units and a ReLU activation function\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    \n",
    "    # Add another Dropout layer\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    \n",
    "    # Add a Dense output layer with a single unit and a sigmoid activation function\n",
    "    # The sigmoid function squashes the output values to be between 0 and 1, which is ideal for binary classification\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Define the loss function to be binary cross entropy\n",
    "# Binary cross entropy is suitable for binary classification problems\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# The model is compiled with the custom loss function (commented out in this code), Adam optimizer, and accuracy as the metric\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size= 128\n",
    "    \n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.keras', monitor='val_loss', save_best_only=False, mode='auto')\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "history = model.fit(\n",
    "    train_x, np.vstack([train_y_meta.loc[:,['y','black']].values]), \n",
    "    batch_size=batch_size, \n",
    "    epochs=num_epochs,\n",
    "    validation_data=(val_x, \n",
    "                     np.vstack([val_y_meta.loc[:,['y','black']].values])),callbacks=[checkpoint, wga])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sequential model\n",
    "model = tf.keras.Sequential([ \n",
    "    # Add an Embedding layer that turns positive integers (indexes) into dense vectors of fixed size\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embeddings], trainable=False),\n",
    "    \n",
    "    # Add a Bidirectional layer with a GRU layer that contains 150 units\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(150, return_sequences=True)),\n",
    "    \n",
    "    # Add a 1D convolution layer with 150 filters, kernel size of 3, same padding, He uniform initializer and ReLU activation function\n",
    "    tf.keras.layers.Conv1D(filters=150, kernel_size=3, padding='same', kernel_initializer='he_uniform', activation='relu'),\n",
    "    \n",
    "    # Add a MaxPooling1D layer with pool size of 3\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=3),\n",
    "    \n",
    "    # Add a GlobalAveragePooling1D layer\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    \n",
    "    # Add a BatchNormalization layer\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # Add a Dense layer with 50 units, ReLU activation function and He uniform initializer\n",
    "    tf.keras.layers.Dense(50, activation='relu', kernel_initializer='he_uniform'),\n",
    "    \n",
    "    # Add a Dropout layer that randomly sets 20% of input units to 0 at each update during training time\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    # Add another Dense layer with 30 units, ReLU activation function and He uniform initializer\n",
    "    tf.keras.layers.Dense(30, activation='relu', kernel_initializer='he_uniform'),\n",
    "    \n",
    "    # Add another Dropout layer\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    # Add a Dense output layer with a single unit, sigmoid activation function and Glorot uniform initializer\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform')\n",
    "])\n",
    "\n",
    "# Define the loss function to be binary cross entropy\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Compile the model with the custom loss function, Adam optimizer, and accuracy as the metric\n",
    "model.compile(loss=[demographic_loss_wrapper('black', bce_loss)], optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 32\n",
    "batch_size= 64\n",
    "    \n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.keras', monitor='val_loss', save_best_only=False, mode='auto')\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    train_x, np.vstack([train_y_meta.loc[:,['y','black']].values]), \n",
    "    batch_size=batch_size, \n",
    "    epochs=num_epochs,\n",
    "    validation_data=(val_x, \n",
    "                     np.vstack([val_y_meta.loc[:,['y','black']].values])),callbacks=[checkpoint, wga])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = str(time.time())\n",
    "path=os.path.join('../models',t)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "model_path=f'{str(round(history.history[\"val_accuracy\"][-1],2))}'\n",
    "export_path = os.path.join(path,model_path)\n",
    "model.save(f'{export_path}.keras')\n",
    "json.dump(history.history,open(f'{export_path}.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_length\": max_length,\n",
    "    \"padding_type\": padding_type,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"trunc_type\": trunc_type,\n",
    "    \"oov_tok\": oov_tok,\n",
    "    'model_accuracy': f'{str(round(history.history[\"val_accuracy\"][-1],2))}'\n",
    "}\n",
    "params_json = json.dumps(params, indent=4)\n",
    "with open(f'{path}/params.json', 'w') as f:\n",
    "    f.write(params_json)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
