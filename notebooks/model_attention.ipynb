{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.layers import Input, Embedding, SpatialDropout1D, Bidirectional,LSTM, GRU, GlobalAveragePooling1D, GlobalMaxPool1D, Concatenate, Add, Dense\n",
    "from tensorflow.keras import Model, regularizers, initializers, constraints\n",
    "import random\n",
    "from tensorflow.keras.layers import Layer\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../params.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "max_length = params['max_length']\n",
    "padding_type = params['padding_type']\n",
    "vocab_size = params['vocab_size']\n",
    "embedding_dim = params['embedding_dim']\n",
    "trunc_type = params['trunc_type']\n",
    "oov_tok = params['oov_tok']\n",
    "embedding_dim=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed data:\n",
    "train_x=np.load('../experiment/processed/train_padded.npy')\n",
    "train_y=np.load('../experiment/processed/train_y.npy')\n",
    "val_x=np.load('../experiment/processed/val_padded.npy')\n",
    "val_y=np.load('../experiment/processed/val_y.npy')\n",
    "train_y_meta=pd.read_csv('../kaggle_data/train_y.csv')\n",
    "val_y_meta=pd.read_csv('../kaggle_data/val_y.csv')\n",
    "word_index=json.load(open('../experiment/processed/word_index.json','r'))\n",
    "train_meta=pd.read_csv('../processed/train_meta.csv').iloc[:,9:15]\n",
    "embed_mat=np.load('../experiment/processed/embedding_matrix_fasttext.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_columns=train_y_meta.columns[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_feat=train_meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wga(y,y_pred):\n",
    "    y.loc[:, 'pred'] = y_pred\n",
    "    categories = ['male', 'female', 'LGBTQ', 'christian', 'muslim', 'other_religions', 'black', 'white']\n",
    "    accuracies = []\n",
    "    groups=[]\n",
    "    for category in categories:\n",
    "        for label in [0, 1]:\n",
    "            group = y.loc[y[category] == label]\n",
    "            group_accuracy = (group['y'] == (group['pred'] > 0.5)).mean()\n",
    "            groups.append(category+'_'+str(label))\n",
    "            accuracies.append(group_accuracy)\n",
    "    wga = np.min(accuracies)\n",
    "    return wga, dict(zip(groups,accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batching_columns(val_x,val_meta,model):\n",
    "    val_x = val_x.reshape((val_x.shape[0], -1))\n",
    "    data=tf.data.Dataset.from_tensor_slices((val_x,val_meta))\n",
    "    data=data.batch(32)\n",
    "\n",
    "    predictions, indices = [], []\n",
    "    for idx, (x, y) in tqdm(enumerate(data), leave=False):\n",
    "        pred = model(x, training=False)\n",
    "        predictions.extend(tf.squeeze(pred).numpy().tolist())\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorstGroupAccuracy(Callback):\n",
    "    def __init__(self, train_data, val_data):\n",
    "        super(WorstGroupAccuracy, self).__init__()\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_x,_,train_meta = self.train_data\n",
    "        val_x,_,val_meta = self.val_data\n",
    "        \n",
    "        # train_y_pred=batching_columns(train_x, train_meta,self.model)\n",
    "        # train_wga, train_metric = calculate_wga(train_meta,train_y_pred)\n",
    "        \n",
    "        val_y_pred=batching_columns(val_x,val_meta,self.model)\n",
    "        val_wga,val_metric = calculate_wga(val_meta,val_y_pred)\n",
    "        \n",
    "        # print(f'{train_wga},Train WGA: {train_metric}')\n",
    "        print(f'{val_wga},Val WGA: {val_metric}')\n",
    "        \n",
    "wga = WorstGroupAccuracy((train_x,train_y,train_meta), (val_x,val_y,val_y_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Basically, hidden state of each timestep is passed through a hidden dense layer of n units, with a softmax layer on top which returns the attention weights for each timestep.\n",
    "    The context vector is then calculated as the attention weighted sum of timestep hidden states.\n",
    "    Input shape\n",
    "        3D tensor with shape: (samples, steps, features).\n",
    "    Output shape\n",
    "        2D tensor with shape: (samples, features).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, units=None,return_weights=False,**kwargs):\n",
    "        tf.random.set_seed(69)\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.return_weights = return_weights\n",
    "        self.units = units\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "        self.supports_masking = True\n",
    "        tf.random.set_seed(69)\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.return_weights = return_weights\n",
    "        self.units = units\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        \n",
    "        if self.units!=None:\n",
    "            num_units = self.units\n",
    "        else:\n",
    "          num_units = input_shape[-1]\n",
    "            \n",
    "        self.W = self.add_weight(name = 'att_W',\n",
    "                             shape =  (input_shape[-1], num_units),\n",
    "                             initializer=self.init,\n",
    "                             regularizer=self.W_regularizer,\n",
    "                             constraint=self.W_constraint,\n",
    "                             trainable=True)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(name = 'att_b',\n",
    "                                 shape = (num_units,),\n",
    "                                 initializer='zero',\n",
    "                                 regularizer=self.b_regularizer,\n",
    "                                 constraint=self.b_constraint,\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(name = 'att_u',\n",
    "                                 shape = (num_units,1),\n",
    "                                 initializer=self.init,\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint,\n",
    "                                 trainable=True)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = tf.matmul(x, self.W) #x(256,).W(256,128) -> uit(128,)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b #uit(128,)+b(128,) -> uit(128,)\n",
    "\n",
    "        uit = tf.keras.activations.tanh(uit) #uit(128,)\n",
    "        ait = tf.matmul(uit, self.u) #uit(128,).u(128,) -> ait(1,)\n",
    "        a = tf.keras.activations.exponential(ait) #a(105,1) for all timesteps\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= tf.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        a /= tf.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx()) \n",
    "\n",
    "        weighted_input = x * a #x(105,256)*a(105,1)\n",
    "        weighted_sum = K.sum(weighted_input,axis=1) #weighted_sum(256,) : context vector\n",
    "        if self.return_weights:\n",
    "            return [weighted_sum, a]\n",
    "        return weighted_sum\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.units!=None:\n",
    "            return input_shape[0], self.units\n",
    "        return input_shape[0], input_shape[-1]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(AttentionWithContext, self).get_config().copy()\n",
    "        config.update({\n",
    "            'W_regularizer': self.W_regularizer,\n",
    "            'u_regularizer': self.u_regularizer,\n",
    "            'b_regularizer': self.b_regularizer,\n",
    "            'W_constraint': self.W_constraint,\n",
    "            'u_constraint': self.u_constraint,\n",
    "            'b_constraint': self.b_constraint,\n",
    "            'bias' : self.bias,\n",
    "            'units': self.units,\n",
    "            'return_weights' : self.return_weights\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall\n",
    "weights = np.ones((len(train_y_meta),)) / 4\n",
    "\n",
    "# Subgroup\n",
    "weights += (train_y_meta[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(int) / 4\n",
    "\n",
    "# Background Positive, Subgroup Negative\n",
    "weights += (( (train_y_meta['y'].values>=0.5).astype(bool).astype(int) +\n",
    "   (train_y_meta[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(int) ) > 1 ).astype(bool).astype(int) / 4\n",
    "\n",
    "# Background Negative, Subgroup Positive\n",
    "weights += (( (train_y_meta['y'].values<0.5).astype(bool).astype(int) +\n",
    "   (train_y_meta[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(int) ) > 1 ).astype(bool).astype(int) / 4\n",
    "\n",
    "# for later normalization the loss\n",
    "loss_weight = 1.0 / weights.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_columns = ['target'] #0/1\n",
    "y_aux_columns = \\\n",
    "['target_prob','target_prob','severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat','sexual_explicit']\n",
    "# two target_prob is for adjusting the weight of aux_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_wrapper(weights):\n",
    "    def tf__custom_loss(y_true, y_pred):\n",
    "        def custom_loss(preds,targets,weights):\n",
    "            bce_loss_1 = nn.BCEWithLogitsLoss(weight=weights)(preds[:,0],targets[:,0]) #weighted y_columns\n",
    "            bce_loss_2 = nn.BCEWithLogitsLoss()(preds[:,1:],targets[:,1:]) # y_aux_columns\n",
    "            return ((bce_loss_1 * loss_weight)*0.60 + bce_loss_2*0.40)*2 \n",
    "        pass\n",
    "    return tf__custom_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sb/anaconda3/envs/dl/lib/python3.9/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/sb/anaconda3/envs/dl/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/sb/anaconda3/envs/dl/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/sb/anaconda3/envs/dl/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/sb/anaconda3/envs/dl/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/sb/anaconda3/envs/dl/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/sb/anaconda3/envs/dl/lib/python3.9/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/sb/anaconda3/envs/dl/lib/python3.9/site-packages/keras/src/losses.py\", line 161, in __call__\n        return losses_utils.compute_weighted_loss(\n    File \"/Users/sb/anaconda3/envs/dl/lib/python3.9/site-packages/keras/src/utils/losses_utils.py\", line 328, in compute_weighted_loss\n        losses = tf.convert_to_tensor(losses)\n\n    ValueError: None values not supported.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./attention/model-\u001b[39m\u001b[38;5;132;01m{epoch:03d}\u001b[39;00m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m'\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     43\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_meta\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwga\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/8w/fc9wlggd18qcqq4x6k8hlvnm0000gn/T/__autograph_generated_file2b3os028.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/sb/anaconda3/envs/dl/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/sb/anaconda3/envs/dl/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/sb/anaconda3/envs/dl/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/sb/anaconda3/envs/dl/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/sb/anaconda3/envs/dl/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/sb/anaconda3/envs/dl/lib/python3.9/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/sb/anaconda3/envs/dl/lib/python3.9/site-packages/keras/src/losses.py\", line 161, in __call__\n        return losses_utils.compute_weighted_loss(\n    File \"/Users/sb/anaconda3/envs/dl/lib/python3.9/site-packages/keras/src/utils/losses_utils.py\", line 328, in compute_weighted_loss\n        losses = tf.convert_to_tensor(losses)\n\n    ValueError: None values not supported.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Config\n",
    "\n",
    "SEED = 69\n",
    "INPUT_LEN = 150 \n",
    "DROPOUT_RATIO = 0.25\n",
    "LSTM_UNITS = 128\n",
    "GRU_UNITS = 128\n",
    "ATT_UNITS = 128\n",
    "DENSE_UNITS = 768\n",
    "NUM_AUX_TARG = len(toxicity_feat)\n",
    "\n",
    "#Architecture\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "sequences = Input(shape = (INPUT_LEN,))\n",
    "x = Embedding(input_dim=embed_mat.shape[0], output_dim=embed_mat.shape[1], input_length = INPUT_LEN, weights=[embed_mat], trainable=False)(sequences)\n",
    "x = SpatialDropout1D(DROPOUT_RATIO)(x)\n",
    "x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "x = Bidirectional(GRU(GRU_UNITS, return_sequences=True))(x)\n",
    "\n",
    "a,w = AttentionWithContext(units=ATT_UNITS, return_weights=True)(x)\n",
    "p1 = GlobalAveragePooling1D()(x)\n",
    "p2 = GlobalMaxPool1D()(x) \n",
    "\n",
    "x = Concatenate()([a,p1,p2])\n",
    "x = Add()([x, Dense(DENSE_UNITS, activation='relu')(x)])\n",
    "x = Add()([x, Dense(DENSE_UNITS, activation='relu')(x)])\n",
    "\n",
    "pred = Dense(1, activation = \"sigmoid\")(x)\n",
    "aux_preds = Dense(NUM_AUX_TARG, activation='sigmoid')(x) \n",
    "\n",
    "model = Model(inputs = sequences, outputs = [pred, aux_preds])\n",
    "\n",
    "#Training\n",
    "model.compile(loss=[custom_loss_wrapper(weights), 'binary_crossentropy'], loss_weights=[loss_weight, 1.0], optimizer=tf.keras.optimizers.Adam(learning_rate=0.0025))\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint('./attention/model-{epoch:03d}.keras', monitor='val_loss', save_best_only=False, mode='auto')\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "\n",
    "history = model.fit(x = train_x, y = [train_y, train_meta], batch_size = 2048, epochs = 5, callbacks = [early_stopping,checkpoint,wga], verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
