{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import itertools\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "from string import ascii_lowercase\n",
    "from functools import reduce\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../params.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "max_length = params['max_length']\n",
    "padding_type = params['padding_type']\n",
    "vocab_size = params['vocab_size']\n",
    "embedding_dim = params['embedding_dim']\n",
    "trunc_type = params['trunc_type']\n",
    "oov_tok = params['oov_tok']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_PATTERNS = {\n",
    "    ' american ':\n",
    "        [\n",
    "            'amerikan'\n",
    "        ],\n",
    "    'america':\n",
    "        [\n",
    "            'amerika'\n",
    "        ],\n",
    "    ' adolf ':\n",
    "        [\n",
    "            'adolf'\n",
    "        ],\n",
    "\n",
    "    ' hitler ':\n",
    "        [\n",
    "            'hitler'\n",
    "        ],\n",
    "\n",
    "    ' fuck':\n",
    "        [\n",
    "            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n",
    "            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n",
    "            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n",
    "            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n",
    "            'feck ', ' fux ', 'f\\*\\*', \n",
    "            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n",
    "        ],\n",
    "\n",
    "    ' ass ':\n",
    "        [\n",
    "            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n",
    "                                                           '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
    "            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n",
    "        ],\n",
    "\n",
    "    ' ass hole ':\n",
    "        [\n",
    "            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole'\n",
    "        ],\n",
    "\n",
    "    ' bitch ':\n",
    "        [\n",
    "            'b[w]*i[t]*ch', 'b!tch',\n",
    "            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
    "            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h'\n",
    "        ],\n",
    "\n",
    "    ' bastard ':\n",
    "        [\n",
    "            'ba[s|z]+t[e|a]+rd'\n",
    "        ],\n",
    "\n",
    "    ' trans gender':\n",
    "        [\n",
    "            'transgender'\n",
    "        ],\n",
    "\n",
    "    ' gay ':\n",
    "        [\n",
    "            'gay'\n",
    "        ],\n",
    "\n",
    "    ' cock ':\n",
    "        [\n",
    "            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n",
    "            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n",
    "        ],\n",
    "\n",
    "    ' dick ':\n",
    "        [\n",
    "            ' dick[^aeiou]', 'deek', 'd i c k'\n",
    "        ],\n",
    "\n",
    "    ' suck ':\n",
    "        [\n",
    "            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n",
    "        ],\n",
    "\n",
    "    ' cunt ':\n",
    "        [\n",
    "            'cunt', 'c u n t'\n",
    "        ],\n",
    "\n",
    "    ' bull shit ':\n",
    "        [\n",
    "            'bullsh\\*t', 'bull\\$hit'\n",
    "        ],\n",
    "\n",
    "    ' homo sex ual':\n",
    "        [\n",
    "            'homosexual'\n",
    "        ],\n",
    "\n",
    "    ' jerk ':\n",
    "        [\n",
    "            'jerk'\n",
    "        ],\n",
    "\n",
    "    ' idiot ':\n",
    "        [\n",
    "            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots'\n",
    "                                                                                      'i d i o t'\n",
    "        ],\n",
    "\n",
    "    ' dumb ':\n",
    "        [\n",
    "            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n",
    "        ],\n",
    "\n",
    "    ' shit ':\n",
    "        [\n",
    "            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t'\n",
    "        ],\n",
    "\n",
    "    ' shit hole ':\n",
    "        [\n",
    "            'shythole'\n",
    "        ],\n",
    "\n",
    "    ' retard ':\n",
    "        [\n",
    "            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n",
    "        ],\n",
    "\n",
    "    ' rape ':\n",
    "        [\n",
    "            ' raped'\n",
    "        ],\n",
    "\n",
    "    ' dumb ass':\n",
    "        [\n",
    "            'dumbass', 'dubass'\n",
    "        ],\n",
    "\n",
    "    ' ass head':\n",
    "        [\n",
    "            'butthead'\n",
    "        ],\n",
    "\n",
    "    ' sex ':\n",
    "        [\n",
    "            'sexy', 's3x', 'sexuality'\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' nigger ':\n",
    "        [\n",
    "            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n",
    "        ],\n",
    "\n",
    "    ' shut the fuck up':\n",
    "        [\n",
    "            'stfu'\n",
    "        ],\n",
    "\n",
    "    ' pussy ':\n",
    "        [\n",
    "            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses'\n",
    "        ],\n",
    "\n",
    "    ' faggot ':\n",
    "        [\n",
    "            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n",
    "            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n",
    "        ],\n",
    "\n",
    "    ' mother fucker':\n",
    "        [\n",
    "            ' motha ', ' motha f', ' mother f', 'motherucker',\n",
    "        ],\n",
    "\n",
    "    ' whore ':\n",
    "        [\n",
    "            'wh\\*\\*\\*', 'w h o r e'\n",
    "        ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../kaggle_data/train_x.csv\").fillna('')\n",
    "val = pd.read_csv(\"../kaggle_data/val_x.csv\").fillna('')\n",
    "test = pd.read_csv(\"../kaggle_data/test_x.csv\").fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_all=pd.read_csv('../kaggle_data/train_y.csv')\n",
    "val_y_all=pd.read_csv('../kaggle_data/val_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(input_text, convert_to_lower=True, remove_patterns=True, remove_repeats=True):\n",
    "    # Convert to lower case\n",
    "    if convert_to_lower:\n",
    "        input_text = input_text.lower()\n",
    "\n",
    "    # Replace patterns\n",
    "    if remove_patterns and RE_PATTERNS:\n",
    "        for replacement, patterns in RE_PATTERNS.items():\n",
    "            for pattern in patterns:\n",
    "                input_text = input_text.replace(pattern, replacement)\n",
    "\n",
    "    # Remove repeated characters\n",
    "    if remove_repeats:\n",
    "        input_text = re.sub(r'(.)\\1{2,}', r'\\1', input_text)\n",
    "\n",
    "    # Replace newline characters with spaces\n",
    "    input_text = input_text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Remove non-alphanumeric characters\n",
    "    input_text = re.sub(r'[^\\w\\s]', ' ', input_text)\n",
    "\n",
    "    # Remove numbers\n",
    "    input_text = re.sub(r'\\d', '', input_text)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    input_text = re.sub(r'\\s+', ' ', input_text)\n",
    "\n",
    "    # Remove non-ASCII characters\n",
    "    input_text = re.sub(r'([^\\x00-\\x7F])+', ' ', input_text)\n",
    "\n",
    "    return input_text.strip()  # Remove leading/trailing spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['string']=train.apply(lambda x: preprocess_text(x['string']),axis=1)\n",
    "val['string']=val.apply(lambda x: preprocess_text(x['string']),axis=1)\n",
    "test['string']=test.apply(lambda x: preprocess_text(x['string']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(input_text, apply_lemmatization=True):\n",
    "    # Initialize the lemmatizer\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # If lemmatization is not required, return the original text\n",
    "    if not apply_lemmatization:\n",
    "        return input_text\n",
    "\n",
    "    # Split the text into words\n",
    "    words = input_text.split()\n",
    "\n",
    "    # Lemmatize each word\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        noun = wordnet_lemmatizer.lemmatize(word, pos=\"n\")\n",
    "        verb = wordnet_lemmatizer.lemmatize(noun, pos=\"v\")\n",
    "        adjective = wordnet_lemmatizer.lemmatize(verb, pos=\"a\")\n",
    "        adverb = wordnet_lemmatizer.lemmatize(adjective, pos=\"r\")\n",
    "        lemmatized_words.append(adverb)\n",
    "\n",
    "    # Join the lemmatized words back into a string\n",
    "    lemmatized_text = \" \".join(lemmatized_words)\n",
    "\n",
    "    return lemmatized_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['string']=train.apply(lambda x: lemmatize_text(x['string']),axis=1)\n",
    "val['string']=val.apply(lambda x: lemmatize_text(x['string']),axis=1)\n",
    "test['string']=test.apply(lambda x: lemmatize_text(x['string']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_strings():\n",
    "    \"\"\"\n",
    "    Generate all possible strings of increasing length using the ASCII lowercase letters.\n",
    "    \"\"\"\n",
    "    for size in itertools.count(1):\n",
    "        for s in itertools.product(ascii_lowercase, repeat=size):\n",
    "            yield \"\".join(s)\n",
    "\n",
    "# List to hold all generated strings\n",
    "generated_strings = []\n",
    "for s in generate_all_strings():\n",
    "    generated_strings.append(s)\n",
    "    if s == 'zz':\n",
    "        break\n",
    "\n",
    "# List of words to remove from the generated strings\n",
    "words_to_remove = ['i', 'a', 'am', 'an', 'as', 'at', 'be', 'by', 'do', 'go', 'he', 'hi', 'if', 'is', 'in', 'me', 'my', 'no', 'of', 'on', 'or', 'ok', 'so', 'to', 'up', 'us', 'we']\n",
    "\n",
    "# Remove the specified words from the list of generated strings\n",
    "for word in words_to_remove:\n",
    "    if word in generated_strings:\n",
    "        generated_strings.remove(word)\n",
    "\n",
    "# Convert the list of generated strings to a set to remove any duplicates\n",
    "stopword_set = set(generated_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_train_data=train['string'].tolist()\n",
    "lemmatized_val_data=val['string'].tolist()\n",
    "lemmatized_test_data=test['string'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_stopwords(data, search_stop=True):\n",
    "    \"\"\"\n",
    "    Function that reads the entire dataset and adds words that are not already present in stopword_set into a list of potential_stopwords.\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    if search_stop:\n",
    "        data = data.split(\" \")\n",
    "        for word in data:\n",
    "            if word not in stopword_set:\n",
    "                output = output + \" \" + word \n",
    "    else:\n",
    "        output = data\n",
    "    return str(output.strip())\n",
    "\n",
    "# Calling the find_non_stopwords function, and saving words into a list (potential_stopwords).\n",
    "potential_stopwords = []\n",
    "for line in tqdm(lemmatized_train_data, total=len(lemmatized_train_data)): \n",
    "    potential_stopwords.append(find_non_stopwords(line))\n",
    "\n",
    "# Functions to combine all the sentences present in potential_stopwords into 4 different strings.\n",
    "# Each function handles a different segment of the data, this facilitates faster concatenation of sentences. \n",
    "def combine_strings(stopwords, start, end):\n",
    "    final_string = \"\"\n",
    "    for item in range(start, end):\n",
    "        final_string = final_string + \" \" + stopwords[item]\n",
    "    return final_string\n",
    "\n",
    "# Combining the potential stopwords into four different strings\n",
    "total_string_potential_a = combine_strings(potential_stopwords, 0, 39893)\n",
    "total_string_potential_b = combine_strings(potential_stopwords, 39893, 79785)\n",
    "total_string_potential_c = combine_strings(potential_stopwords, 79785, 119678)\n",
    "total_string_potential_d = combine_strings(potential_stopwords, 119678, 159571)\n",
    "\n",
    "def count_words(str):\n",
    "    \"\"\"\n",
    "    Function to count the number of words in a string and save it in a dictionary.\n",
    "    \"\"\"\n",
    "    counts = dict()\n",
    "    words = str.split()\n",
    "\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "    return counts\n",
    "\n",
    "# Counting the number of words in each of the 4 strings\n",
    "total_string_potential_a_dict = count_words(total_string_potential_a)\n",
    "total_string_potential_b_dict = count_words(total_string_potential_b)\n",
    "total_string_potential_c_dict = count_words(total_string_potential_c)\n",
    "total_string_potential_d_dict = count_words(total_string_potential_d)\n",
    "\n",
    "# Converting Dictionaries to Dataframes.\n",
    "def dict_to_df(dict):\n",
    "    return pd.DataFrame(list(dict.items()),columns = ['Word','Count'])\n",
    "\n",
    "total_string_potential_a_df = dict_to_df(total_string_potential_a_dict)\n",
    "total_string_potential_b_df = dict_to_df(total_string_potential_b_dict)\n",
    "total_string_potential_c_df = dict_to_df(total_string_potential_c_dict)\n",
    "total_string_potential_d_df = dict_to_df(total_string_potential_d_dict)\n",
    "\n",
    "# Getting Dataframe output in descending order.\n",
    "def get_top50(df):\n",
    "    return df.sort_values(by=['Count'],ascending=False).head(50)\n",
    "\n",
    "top50_potential_stopwords_a = get_top50(total_string_potential_a_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_stopwords(data, search_stop=True):\n",
    "    \"\"\"\n",
    "    Function that reads the entire dataset and adds words that are not already present in stopword_set into a list of potential_stopwords.\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    if search_stop:\n",
    "        data = data.split(\" \")\n",
    "        for word in data:\n",
    "            if word not in stopword_set:\n",
    "                output = output + \" \" + word \n",
    "    else:\n",
    "        output = data\n",
    "    return str(output.strip())\n",
    "\n",
    "# Calling the find_non_stopwords function, and saving words into a list (potential_stopwords).\n",
    "potential_stopwords = []\n",
    "for line in tqdm(lemmatized_train_data, total=len(lemmatized_train_data)): \n",
    "    potential_stopwords.append(find_non_stopwords(line))\n",
    "\n",
    "# Functions to combine all the sentences present in potential_stopwords into 4 different strings.\n",
    "# Each function handles a different segment of the data, this facilitates faster concatenation of sentences. \n",
    "def combine_strings(stopwords, start, end):\n",
    "    final_string = \"\"\n",
    "    for item in range(start, end):\n",
    "        final_string = final_string + \" \" + stopwords[item]\n",
    "    return final_string\n",
    "\n",
    "# Combining the potential stopwords into four different strings\n",
    "total_string_potential_a = combine_strings(potential_stopwords, 0, 39893)\n",
    "total_string_potential_b = combine_strings(potential_stopwords, 39893, 79785)\n",
    "total_string_potential_c = combine_strings(potential_stopwords, 79785, 119678)\n",
    "total_string_potential_d = combine_strings(potential_stopwords, 119678, 159571)\n",
    "\n",
    "def count_words(str):\n",
    "    \"\"\"\n",
    "    Function to count the number of words in a string and save it in a dictionary.\n",
    "    \"\"\"\n",
    "    counts = dict()\n",
    "    words = str.split()\n",
    "\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "    return counts\n",
    "\n",
    "# Counting the number of words in each of the 4 strings\n",
    "total_string_potential_a_dict = count_words(total_string_potential_a)\n",
    "total_string_potential_b_dict = count_words(total_string_potential_b)\n",
    "total_string_potential_c_dict = count_words(total_string_potential_c)\n",
    "total_string_potential_d_dict = count_words(total_string_potential_d)\n",
    "\n",
    "# Converting Dictionaries to Dataframes.\n",
    "def dict_to_df(dict):\n",
    "    return pd.DataFrame(list(dict.items()),columns = ['Word','Count'])\n",
    "\n",
    "total_string_potential_a_df = dict_to_df(total_string_potential_a_dict)\n",
    "total_string_potential_b_df = dict_to_df(total_string_potential_b_dict)\n",
    "total_string_potential_c_df = dict_to_df(total_string_potential_c_dict)\n",
    "total_string_potential_d_df = dict_to_df(total_string_potential_d_dict)\n",
    "\n",
    "# Getting Dataframe output in descending order.\n",
    "def get_top50(df):\n",
    "    return df.sort_values(by=['Count'],ascending=False).head(50)\n",
    "\n",
    "top50_potential_stopwords_a = get_top50(total_string_potential_a_df)\n",
    "top50_potential_stopwords_b = get_top50(total_string_potential_b_df)\n",
    "top50_potential_stopwords_c = get_top50(total_string_potential_c_df)\n",
    "top50_potential_stopwords_d = get_top50(total_string_potential_d_df)\n",
    "\n",
    "# Looking for common terms in all top 50 dataframes.\n",
    "common_potential_stopwords = list(reduce(set.intersection, map(set, [top50_potential_stopwords_a.Word, top50_potential_stopwords_b.Word, top50_potential_stopwords_c.Word, top50_potential_stopwords_d.Word])))\n",
    "\n",
    "# Retaining certain words and removing others from the above list.\n",
    "retained_stopwords = ['editor', 'reference', 'thank', 'work','find', 'good', 'know', 'like', 'look', 'thing', 'want', 'time', 'list', 'section','wikipedia', 'doe', 'add','new', 'try', 'think', 'write','use', 'user', 'way', 'page']\n",
    "\n",
    "# Adding above retrieved words into the stopwords set.\n",
    "for word in retained_stopwords:\n",
    "    stopword_set.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=vocab_size\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train['string']))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(train['string'])\n",
    "list_tokenized_val = tokenizer.texts_to_sequences(val['string'])\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(test['string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = tokenizer.word_counts\n",
    "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "top_n_words = vocab_size - 1 \n",
    "\n",
    "top_words_indices = {word: index for index, (word, _) in enumerate(sorted_word_counts[:top_n_words])}\n",
    "filtered_word_index = {word: index for word, index in tokenizer.word_index.items() if word in top_words_indices}\n",
    "\n",
    "print(\"Filtered Word Index:\", len(filtered_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpadlen = max_length\n",
    "train_padded=pad_sequences(list_tokenized_train, maxlen=maxpadlen, padding = 'post')\n",
    "val_padded=pad_sequences(list_tokenized_val, maxlen=maxpadlen, padding = 'post')\n",
    "test_padded=pad_sequences(list_tokenized_test, maxlen=maxpadlen, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim_fasttext = embedding_dim\n",
    "embeddings_index_fasttext = {}\n",
    "f = open('../wiki-news-300d-1M.vec', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    embeddings_index_fasttext[word] = np.asarray(values[1:], dtype='float32')\n",
    "f.close()\n",
    "embedding_matrix_fasttext = np.random.random((len(filtered_word_index) + 1, embedding_dim_fasttext))\n",
    "for word, i in filtered_word_index.items():\n",
    "    embedding_vector = embeddings_index_fasttext.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_fasttext[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=np.array(train_y_all['y'])\n",
    "val_y=np.array(val_y_all['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./processed/train_padded.npy', 'wb') as f:\n",
    "    np.save(f, train_padded)\n",
    "with open('./processed/val_padded.npy', 'wb') as f:\n",
    "    np.save(f, val_padded)\n",
    "with open('./processed/test_padded.npy', 'wb') as f:\n",
    "    np.save(f, test_padded)\n",
    "with open('./processed/train_y.npy', 'wb') as f:\n",
    "    np.save(f, train_y)\n",
    "with open('./processed/val_y.npy', 'wb') as f:\n",
    "    np.save(f, val_y)\n",
    "with open('./processed/word_index.json', 'w') as f:\n",
    "    json.dump(filtered_word_index, f)\n",
    "with open('./processed/embedding_matrix_fasttext.npy', 'wb') as f:\n",
    "    np.save(f, embedding_matrix_fasttext)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
